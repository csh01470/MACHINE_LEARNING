{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH03.3. **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 여러 약한 모델(학습기)을 순차적으로 학습시켜 강한 모델을 만드는 앙상블 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "#### $ \\hspace{0.15cm} $ ① 순차적 학습 : 이전 단계의 결과를 이용하여 다음 학습기를 학습함\n",
    "#### $ \\hspace{0.15cm} $ ② 오류 보완 : 이전 단계의 약한 모델의 오류를 보완하여 다음 모델이 이를 집중적으로 학습할 수 있도록 처리함\n",
    "#### $ \\hspace{0.15cm} $ ③ 특성 중요도(Feature Importance) 추출 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "#### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려\n",
    "#### $ \\hspace{0.15cm} $ ② 데이터 노이즈(noise)에 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(5) 종류** :\n",
    "#### $ \\hspace{0.15cm} $ ① AdaBoost\n",
    "#### $ \\hspace{0.15cm} $ ② Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **AdaBoost(Adaptive Boost)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 이전 학습기에서 잘못 분류된 데이터(샘플)의 추출 가중치를 증가함으로써 오류에 대한 민감성을 줄이는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "#### $ \\hspace{0.15cm} $ ① 단순한 약한 학습기 사용 : 아주 간단한 모델(스텀프;$  1 $ 회 분기처리한 트리)을 사용하여 복잡성을 관리\n",
    "#### $ \\hspace{0.15cm} $ ② 가중치 조정 : 각 데이터 포인트에 가중치를 부여하여, 잘못 분류된 데이터를 더 많이 샘플링함 \n",
    "#### $ \\hspace{0.15cm} $ ③ 가중 합산 : 최종 예측은 각 약한 학습기의 예측에 가중치를 부여하여 합산한 결과로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "#### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려\n",
    "#### $ \\hspace{0.15cm} $ ② 데이터 노이즈(noise;$ \\epsilon{} $)에 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) Pseudo Code** :\n",
    "```python\n",
    "def AdaBoost(data, labels, num_iterations) :\n",
    "    weights = [1/N] * N  # N: number of samples\n",
    "    classifiers = []\n",
    "    classifier_weights = []\n",
    "\n",
    "    for t in range(num_iterations) :\n",
    "        weak_classifier = TrainWeakClassifier(data, labels, weights)\n",
    "        predictions = weak_classifier.predict(data)\n",
    "        error = sum(weights[i] for i where predictions[i] != labels[i])\n",
    "\n",
    "        if error > 0.5 :\n",
    "            break\n",
    "\n",
    "        alpha = 0.5 * ln((1 - error) / error)\n",
    "        classifier_weights.append(alpha)\n",
    "        classifiers.append(weak_classifier)\n",
    "\n",
    "        for i in range(N) :\n",
    "            weights[i] *= exp(-alpha * labels[i] * predictions[i])\n",
    "        \n",
    "        Normalize(weights)\n",
    "    \n",
    "    return classifiers, classifier_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 손실 함수의 그레이디언트를 사용하여 새로운 약한 모델을 추가하며 전체 모델의 성능을 점진적으로 향상시키는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "#### $ \\hspace{0.15cm} $ ① 단순한 약한 학습기 사용 : 간단한 모델을 사용하여 복잡성을 관리\n",
    "#### $ \\hspace{0.15cm} $ ② 손실 함수 최적화 : 특정 손실 함수를 최소화하는 방향으로 모델을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "#### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려 : 약한 모델이 많아질수록 노이즈까지 학습할 가능성이 높음\n",
    "#### $ \\hspace{0.15cm} $ ② 하이퍼파라미터 조정 복잡 : 다양한 하이퍼파라미터를 조정해야 하므로 복잡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) 과적합 방지를 위한 정규화(regularization)** :\n",
    "#### $ \\hspace{0.15cm} \\cdot{} $ sub-sampling : 약한 모델별로 원본 데이터셋 무작위로 (복원, 비복원 상관없이) 추출하여 학습\n",
    "#### $ \\hspace{0.15cm} \\cdot{} $ shirinkage : 약한 모델의 취합할 때 후속 모델일수록 가중치를 낮게 잡아 예측값 산출\n",
    "#### $ \\hspace{0.45cm} \\text{ex. } \\; \\hat{y} = (0.9)^{0}f_{1} + (0.9)^{1}f_{2} + (0.9)^{2}f_{3} + \\cdots{} + (0.9)^{n-1}f_{n} $ \n",
    "#### $ \\hspace{0.15cm} \\cdot{} $ early-stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) Pseudo Code** :\n",
    "```python\n",
    "def GradientBoosting(data, labels, num_iterations, learning_rate, max_depth, loss_function) :\n",
    "    F_0 = initial_prediction(labels, loss_function)\n",
    "    F = [F_0 for _ in range(len(labels))]\n",
    "    trees = []\n",
    "\n",
    "    for m in range(num_iterations) :\n",
    "        # Compute the gradient (residuals) of the loss function\n",
    "        residuals = compute_gradient(labels, F, loss_function)\n",
    "        \n",
    "        # Train a decision tree targeting the residuals\n",
    "        tree = DecisionTree(data, residuals, max_depth)\n",
    "        \n",
    "        # Calculate the tree's predictions\n",
    "        update = tree.predict(data)\n",
    "        \n",
    "        # Update the model (element-wise addition)\n",
    "        F = [f + learning_rate * u for f, u in zip(F, update)]\n",
    "        \n",
    "        # Store the trained tree\n",
    "        trees.append(tree)\n",
    "    \n",
    "    # Return the final model\n",
    "    return trees, F_0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
